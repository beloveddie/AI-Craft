{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3153d1-180b-4523-aa98-b70901462c1d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eab42-6550-4eb3-aba5-acc7935b68f8",
   "metadata": {},
   "source": [
    "### Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3a7e2-302c-4bbe-b1e2-1e8c9c42cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install google-cloud-bigquery==3.25.0 -U\n",
    "!pip install google-cloud-aiplatform==1.59.0\n",
    "!pip uninstall -y shapely pygeos geopandas\n",
    "!pip install shapely==1.8.5.post1 pygeos==0.12.0 geopandas>=0.12.2\n",
    "# Install pydot and graphviz\n",
    "!pip install pydot\n",
    "!sudo apt install graphviz -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919994b5-c858-4caf-a953-61802d611710",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.15.0 tensorflow-hub==0.15.0 tensorflow-text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b2bef-e04c-48cf-b584-6835a16fab1d",
   "metadata": {},
   "source": [
    "### Restart Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed535aa7-2d3e-415e-b951-e3a9302d9643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981547a-1e6f-4ea7-94c7-f819a1a6c1db",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e74337-7eb7-4102-88d0-61d80b97d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add installed library dependencies to Python PATH variable.\n",
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c08c5-d0cc-431e-a5c7-d49ffb9aa62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and set PROJECT_ID and REGION environment variables.\n",
    "# TODO: Fill in the PROJECT_ID and REGION provided in the lab manual.\n",
    "PROJECT_ID = \"...\"\n",
    "REGION = \"us-east1\"\n",
    "GCS_BUCKET = f\"gs://{PROJECT_ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60a17e-0c17-44c2-aee3-a079be2e782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage buckets create -l $REGION $GCS_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0349709c-b0ff-484f-a60b-6f497d2db788",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f98d50-4af6-457e-9c63-f2685355d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "# TensorFlow model building libraries.\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Re-create the AdamW optimizer used in the original BERT paper.\n",
    "from official.nlp import optimization  \n",
    "\n",
    "# Libraries for data and plot model training metrics.\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the Vertex AI Python SDK.\n",
    "from google.cloud import aiplatform as vertexai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d106afa4-b6c8-4f64-ad3e-ecfda6a441d1",
   "metadata": {},
   "source": [
    "### Initialize Vertex AI Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902df1f5-7bc0-4cda-84f1-6368109c3460",
   "metadata": {},
   "source": [
    "Initialize the Vertex AI Python SDK with your GCP Project, Region, and Google Cloud Storage Bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2f27bf-cacc-416a-80f2-77d991e758ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertexai.init(project=PROJECT_ID, location=REGION, staging_bucket=GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46142418-0cc7-4e6b-b0a9-6c40d1db7c7b",
   "metadata": {},
   "source": [
    "## Build and train your model locally in a Vertex Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b6060-1f2d-4dbf-912c-afafbb85bde7",
   "metadata": {},
   "source": [
    "### Lab Dataset\n",
    "\n",
    "In this lab, we will use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment) that contains the text of 50,000 movie reviews from the Internet Movie Database. These are split into 25,000 reviews for training and 25,000 reviews for testing. The training and testing sets are balanced, meaning they contain an equal number of positive and negative reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa746a78-9ba5-4f44-a44b-f3f0fd37848e",
   "metadata": {},
   "source": [
    "### Data ingestion and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5998b9f-fe74-44cd-aa93-b5eab2edca18",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694ee01-6578-4636-a4d9-a9e4878e4a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "LOCAL_DATA_DIR = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8d17e5-ddbb-4ce8-aab6-1b6a443099ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(data_url, local_data_dir):\n",
    "    \"\"\"Download dataset.\n",
    "    Args:\n",
    "      data_url(str): Source data URL path.\n",
    "      local_data_dir(str): Local data download directory path.\n",
    "    Returns:\n",
    "      dataset_dir(str): Local unpacked data directory path.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_data_dir):\n",
    "        os.makedirs(local_data_dir)\n",
    "    \n",
    "    dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb_v1.tar.gz\",\n",
    "      origin=data_url,\n",
    "      untar=True,\n",
    "      cache_dir=local_data_dir,\n",
    "      cache_subdir=\"\")\n",
    "    \n",
    "    dataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
    "    \n",
    "    train_dir = os.path.join(dataset_dir, \"train\")\n",
    "    \n",
    "    # Remove unused folders to make it easier to load the data.\n",
    "    remove_dir = os.path.join(train_dir, \"unsup\") # commented out to resolve bug\n",
    "    shutil.rmtree(remove_dir)\n",
    "    \n",
    "    return dataset_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fc7a2-30e9-4ecf-87fd-f3020e99c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = download_data(data_url=DATA_URL, local_data_dir=LOCAL_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c47f7-b577-4f25-b1d6-e8fb917a4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to iteratively add data pipeline and model training hyperparameters.\n",
    "HPARAMS = {\n",
    "    # Set a random sampling seed to prevent data leakage in data splits from files.\n",
    "    \"seed\": 42,\n",
    "    # Number of training and inference examples.\n",
    "    \"batch-size\": 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b63266-819a-4d79-a900-eb488abd19ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(dataset_dir, hparams):\n",
    "    \"\"\"Load pre-split tf.datasets.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      raw_train_ds(tf.dataset): Train split dataset (20k examples).\n",
    "      raw_val_ds(tf.dataset): Validation split dataset (5k examples).\n",
    "      raw_test_ds(tf.dataset): Test split dataset (25k examples).\n",
    "    \"\"\"    \n",
    "\n",
    "    raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'train'),\n",
    "        batch_size=hparams['batch-size'],\n",
    "        validation_split=0.2,\n",
    "        subset='training',\n",
    "        seed=hparams['seed'])    \n",
    "\n",
    "    raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'train'),\n",
    "        batch_size=hparams['batch-size'],\n",
    "        validation_split=0.2,\n",
    "        subset='validation',\n",
    "        seed=hparams['seed'])\n",
    "\n",
    "    raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "        os.path.join(dataset_dir, 'test'),\n",
    "        batch_size=hparams['batch-size'])\n",
    "    \n",
    "    return raw_train_ds, raw_val_ds, raw_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae77a4b-a475-4cdb-9895-b3f87a7fe725",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds, raw_val_ds, raw_test_ds = load_datasets(DATASET_DIR, HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4d370-cd2e-4296-88b6-0f272d3e521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "CLASS_NAMES = raw_train_ds.class_names\n",
    "\n",
    "train_ds = raw_train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = raw_val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = raw_test_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408278d8-8998-4e6b-ab0a-97ba1c446291",
   "metadata": {},
   "source": [
    "Let's print a few example reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ea7857-50a4-4e08-9173-716525d5d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "  for i in range(3):\n",
    "    print(f'Review {i}: {text_batch.numpy()[i]}')\n",
    "    label = label_batch.numpy()[i]\n",
    "    print(f'Label : {label} ({CLASS_NAMES[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79523ac6-ee1c-4ba0-970f-7d3d9d4b78cb",
   "metadata": {},
   "source": [
    "### Choose a pre-trained BERT model to fine-tune for higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68306ae9-e3ff-4159-a240-fcfc989a560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPARAMS.update({\n",
    "    # TF Hub BERT modules.\n",
    "    \"tfhub-bert-preprocessor\": \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "    \"tfhub-bert-encoder\": \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e980d93-ae65-4b84-aea3-b91c991a7e90",
   "metadata": {},
   "source": [
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. Since this text preprocessor is a TensorFlow model, It can be included in your model directly.\n",
    "\n",
    "For fine-tuning, you will use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "For the learning rate `initial-learning-rate`, you will use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps `n_warmup_steps`. In line with the BERT paper, the initial learning rate is smaller for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f8419d-e9a9-41e1-8eeb-f8b789647ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPARAMS.update({\n",
    "    # Model training hyperparameters for fine tuning and regularization.\n",
    "    \"epochs\": 5,\n",
    "    \"initial-learning-rate\": 3e-5,\n",
    "    \"dropout\": 0.1 \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533a5db2-69c9-4a28-9fb1-951ba65c4cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n",
    "# https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c921c-c92e-4bb1-b936-38aeaafe8e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPARAMS.update({\n",
    "    'model-dir': './saved_model'  # Add this line\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93683af5-f110-4994-8957-25e563a3be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954687cb-30d2-436f-ab37-23aaa6082ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "HPARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e444d-6c5e-4f08-8286-a5b2cd67fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training steps\n",
    "epochs = HPARAMS['epochs']\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "n_train_steps = steps_per_epoch * epochs\n",
    "n_warmup_steps = int(0.1 * n_train_steps)    \n",
    "\n",
    "# Create learning rate schedule with warmup\n",
    "class WarmupLinearDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, decay_steps, warmup_steps):\n",
    "        super().__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # Cast to float32 to avoid dtype issues\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        decay_steps = tf.cast(self.decay_steps, tf.float32)\n",
    "        \n",
    "        # Warmup phase: linear increase from 0 to initial_lr\n",
    "        warmup_lr = self.initial_learning_rate * step / warmup_steps\n",
    "        \n",
    "        # Decay phase: linear decrease from initial_lr to 0\n",
    "        decay_lr = self.initial_learning_rate * (\n",
    "            1.0 - (step - warmup_steps) / (decay_steps - warmup_steps)\n",
    "        )\n",
    "        \n",
    "        # Choose between warmup and decay\n",
    "        return tf.cond(\n",
    "            step < warmup_steps,\n",
    "            lambda: warmup_lr,\n",
    "            lambda: tf.maximum(decay_lr, 0.0)  # Don't go below 0\n",
    "        )\n",
    "\n",
    "# Create the learning rate schedule\n",
    "lr_schedule = WarmupLinearDecay(\n",
    "    initial_learning_rate=HPARAMS['initial-learning-rate'],\n",
    "    decay_steps=n_train_steps,\n",
    "    warmup_steps=n_warmup_steps\n",
    ")\n",
    "\n",
    "# Create optimizer with warmup schedule\n",
    "OPTIMIZER = tf.keras.optimizers.AdamW(\n",
    "    learning_rate=lr_schedule,\n",
    "    weight_decay=0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7183ae-fb2a-40de-aa29-f4bf1c98a338",
   "metadata": {},
   "source": [
    "### Build and compile a TensorFlow BERT sentiment classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc812a6-3948-4723-a493-f069f1d5a90d",
   "metadata": {},
   "source": [
    "Next, we will define and compile our model by assembling pre-built TF-Hub components and tf.keras layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2a6e8c-bb4a-4444-b8dd-e553c77e107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_text_classifier(hparams, optimizer):\n",
    "    \"\"\"Define and compile a TensorFlow BERT sentiment classifier.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      model(tf.keras.Model): A compiled TensorFlow model.\n",
    "    \"\"\"\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessor = hub.KerasLayer(hparams['tfhub-bert-preprocessor'], name='preprocessing')\n",
    "    \n",
    "    \n",
    "    encoder_inputs = preprocessor(text_input)\n",
    "    encoder = hub.KerasLayer(hparams['tfhub-bert-encoder'], trainable=True, name='BERT_encoder')\n",
    "\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    # For the fine-tuning you are going to use the `pooled_output` array which represents \n",
    "    # each input sequence as a whole. The shape is [batch_size, H]. \n",
    "    # You can think of this as an embedding for the entire movie review.\n",
    "    classifier = outputs['pooled_output']\n",
    "    # Add dropout to prevent overfitting during model fine-tuning.\n",
    "    classifier = tf.keras.layers.Dropout(hparams['dropout'], name='dropout')(classifier)\n",
    "    classifier = tf.keras.layers.Dense(1, activation=None, name='classifier')(classifier)\n",
    "    model = tf.keras.Model(text_input, classifier, name='bert-sentiment-classifier')\n",
    "    \n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    metrics = tf.metrics.BinaryAccuracy()    \n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss,\n",
    "                  metrics=metrics)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b678a0-37fc-48b2-a32a-0f8ea5261dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_text_classifier(HPARAMS, OPTIMIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded859be-e744-4f65-a001-9839a22f0d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize your fine-tuned BERT sentiment classifier.\n",
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c917fc-c8d1-419c-884f-6e12f53abef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_REVIEW = ['the passion of CHRIST is such an amazing movie!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae408b-c33e-4140-ade6-86a1a59cc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_RAW_RESULT = model(tf.constant(TEST_REVIEW))\n",
    "print(BERT_RAW_RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611df50c-17f0-452c-a754-673ceadcd6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_REVIEW_0 = ['this is such an amazing movie!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d18f733-2d54-4b2c-8ab3-e5f37f38bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_RAW_RESULT = model(tf.constant(TEST_REVIEW_0))\n",
    "print(BERT_RAW_RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e422934c-0896-4cd5-a843-c5ec7cad5f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate(hparams):\n",
    "    \"\"\"Train and evaluate TensorFlow BERT sentiment classifier.\n",
    "    Args:\n",
    "      hparams(dict): A dictionary containing model training arguments.\n",
    "    Returns:\n",
    "      history(tf.keras.callbacks.History): Keras callback that records training event history.\n",
    "    \"\"\"\n",
    "    # dataset_dir = download_data(data_url, local_data_dir)\n",
    "    raw_train_ds, raw_val_ds, raw_test_ds = load_datasets(DATASET_DIR, hparams)\n",
    "    \n",
    "    train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)     \n",
    "    \n",
    "    epochs = hparams['epochs']\n",
    "    steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "    n_train_steps = steps_per_epoch * epochs\n",
    "    n_warmup_steps = int(0.1 * n_train_steps)    \n",
    "    \n",
    "    # Create learning rate schedule with warmup\n",
    "    class WarmupLinearDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, initial_learning_rate, decay_steps, warmup_steps):\n",
    "            super().__init__()\n",
    "            self.initial_learning_rate = initial_learning_rate\n",
    "            self.decay_steps = decay_steps\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            # Cast to float32 to avoid dtype issues\n",
    "            step = tf.cast(step, tf.float32)\n",
    "            warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "            decay_steps = tf.cast(self.decay_steps, tf.float32)\n",
    "            \n",
    "            # Warmup phase: linear increase from 0 to initial_lr\n",
    "            warmup_lr = self.initial_learning_rate * step / warmup_steps\n",
    "            \n",
    "            # Decay phase: linear decrease from initial_lr to 0\n",
    "            decay_lr = self.initial_learning_rate * (\n",
    "                1.0 - (step - warmup_steps) / (decay_steps - warmup_steps)\n",
    "            )\n",
    "            \n",
    "            # Choose between warmup and decay\n",
    "            return tf.cond(\n",
    "                step < warmup_steps,\n",
    "                lambda: warmup_lr,\n",
    "                lambda: tf.maximum(decay_lr, 0.0)  # Don't go below 0\n",
    "            )\n",
    "\n",
    "    # Create the learning rate schedule\n",
    "    lr_schedule = WarmupLinearDecay(\n",
    "        initial_learning_rate=hparams['initial-learning-rate'],\n",
    "        decay_steps=n_train_steps,\n",
    "        warmup_steps=n_warmup_steps\n",
    "    )\n",
    "\n",
    "    # Create optimizer with warmup schedule\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=lr_schedule,\n",
    "        weight_decay=0.01,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-6\n",
    "    )\n",
    "    \n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "        model = build_text_classifier(hparams=hparams, optimizer=optimizer)\n",
    "    \n",
    "    logging.info(model.summary())\n",
    "        \n",
    "    history = model.fit(x=train_ds,\n",
    "                        validation_data=val_ds,\n",
    "                        epochs=epochs)  \n",
    "    \n",
    "    logging.info(\"Test accuracy: %s\", model.evaluate(test_ds))\n",
    "    # Export Keras model in TensorFlow SavedModel format.\n",
    "    model.save(hparams['model-dir'])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4414c-038b-4a4a-87f8-de95ee3ca8d5",
   "metadata": {},
   "source": [
    "Based on the `History` object returned by `model.fit()`. We can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950a959-42b5-4be0-9103-ffa4f86940a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = train_evaluate(HPARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd4cbd-fadd-452f-9d93-1f5051aba177",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a0384c-67e3-457d-b45f-c2aa4e34f68a",
   "metadata": {},
   "source": [
    "In this plot, the red lines represent the training loss and accuracy, and the blue lines are the validation loss and accuracy. Based on the plots above, we should see model accuracy of around 78-80% which exceeds our business requirements target of greater than 75% accuracy."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
