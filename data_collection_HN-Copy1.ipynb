{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1016545d-d72c-4e61-ba16-88f574fa47d8",
   "metadata": {},
   "source": [
    "## Install Dependencies and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a67beb3-53d8-4c80-8add-928bd39aa68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests google-cloud-storage pandas python-dotenv beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13e5c0-37cd-4930-b5c3-71d018e196ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from google.cloud import storage\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging for Jupyter\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")\n",
    "print(\"üî• Ready to collect high-quality tech discourse from Hacker News!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb9719f-0bcd-4ae3-ad25-fefe6960f5ae",
   "metadata": {},
   "source": [
    "## Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e481fb-5ce6-4f38-9e0e-ee761c07d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacker News API Configuration (no authentication needed!)\n",
    "HN_API_BASE = \"https://hacker-news.firebaseio.com/v0\"\n",
    "\n",
    "# GCP Configuration \n",
    "GCP_CONFIG = {\n",
    "    'project_id': '...',  # Your project ID\n",
    "    'bucket_name': '...',  # Your bucket name\n",
    "    'credentials_path': None  # Set if using service account key file\n",
    "}\n",
    "\n",
    "# Collection Parameters\n",
    "COLLECTION_PARAMS = {\n",
    "    'max_stories': 200,          # Number of stories to check\n",
    "    'max_comments_per_story': 20, # Comments per relevant story\n",
    "    'min_comment_length': 15,     # Minimum comment character length\n",
    "    'max_comment_depth': 3,       # How deep in comment threads to go\n",
    "    'stories_lookback_hours': 168 # Look back 1 week (168 hours)\n",
    "}\n",
    "\n",
    "# OpenAI-related keywords (expanded for HN tech discourse)\n",
    "OPENAI_KEYWORDS = [\n",
    "    'openai', 'chatgpt', 'gpt-4', 'gpt-3', 'gpt', 'dall-e', 'dalle',\n",
    "    'sam altman', 'artificial general intelligence', 'agi',\n",
    "    'large language model', 'llm', 'transformer', 'generative ai',\n",
    "    'artificial intelligence', 'machine learning ml', 'neural network',\n",
    "    'deep learning', 'natural language processing', 'nlp',\n",
    "    'prompt engineering', 'fine-tuning', 'ai safety', 'alignment',\n",
    "    'microsoft openai', 'github copilot', 'ai assistant'\n",
    "]\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration complete!\")\n",
    "print(f\"üéØ Targeting stories with keywords: {', '.join(OPENAI_KEYWORDS[:5])}...\")\n",
    "print(f\"üìä Will collect up to {COLLECTION_PARAMS['max_stories']} stories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e49653-eadd-4253-9eff-eaa31d0e6103",
   "metadata": {},
   "source": [
    "## Hacker News API Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408c247-0526-476f-b23f-14c5faa5140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HackerNewsAPI:\n",
    "    \"\"\"Client for interacting with Hacker News API\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = HN_API_BASE):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'OpenAI-Sentiment-Collector/1.0'\n",
    "        })\n",
    "        \n",
    "    def get_item(self, item_id: int) -> Optional[Dict]:\n",
    "        \"\"\"Get a single item (story, comment, etc.) by ID\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/item/{item_id}.json\")\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching item {item_id}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_top_stories(self, limit: int = 500) -> List[int]:\n",
    "        \"\"\"Get top story IDs\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/topstories.json\")\n",
    "            response.raise_for_status()\n",
    "            story_ids = response.json()\n",
    "            return story_ids[:limit]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching top stories: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_new_stories(self, limit: int = 500) -> List[int]:\n",
    "        \"\"\"Get new story IDs\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/newstories.json\")\n",
    "            response.raise_for_status()\n",
    "            story_ids = response.json()\n",
    "            return story_ids[:limit]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching new stories: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_best_stories(self, limit: int = 200) -> List[int]:\n",
    "        \"\"\"Get best story IDs\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(f\"{self.base_url}/beststories.json\")\n",
    "            response.raise_for_status()\n",
    "            story_ids = response.json()\n",
    "            return story_ids[:limit]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching best stories: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba0c54-1d02-4a96-b997-45090b9f39e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize API client\n",
    "hn_api = HackerNewsAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3bd21a-0f06-41ad-abc6-54a0b3ffbbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection\n",
    "print(\"üß™ Testing Hacker News API connection...\")\n",
    "test_item = hn_api.get_item(1)  # Get the first HN item ever\n",
    "if test_item:\n",
    "    print(f\"‚úÖ API connection successful!\")\n",
    "    print(f\"   First HN item: '{test_item.get('title', 'No title')}' by {test_item.get('by', 'unknown')}\")\n",
    "else:\n",
    "    print(\"‚ùå API connection failed\")\n",
    "\n",
    "print(\"üîß Hacker News API client ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f6ec6-c8ed-40eb-990a-58a9e3fd0b06",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85947ffb-9d65-4d3e-b165-6e62d35ebd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_openai_keywords(text: str) -> bool:\n",
    "    \"\"\"Check if text contains OpenAI-related keywords\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    text_lower = text.lower()\n",
    "    return any(keyword in text_lower for keyword in OPENAI_KEYWORDS)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and preprocess text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove HTML entities and tags if present\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove excessive whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    # Remove common HN artifacts\n",
    "    text = re.sub(r'\\[flagged\\]', '', text)\n",
    "    text = re.sub(r'\\[dead\\]', '', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def extract_story_data(story_item: Dict) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Extract relevant data from a HN story\"\"\"\n",
    "    try:\n",
    "        if not story_item or story_item.get('deleted') or story_item.get('dead'):\n",
    "            return None\n",
    "            \n",
    "        # Get story text content\n",
    "        title = story_item.get('title', '')\n",
    "        text = story_item.get('text', '')\n",
    "        url = story_item.get('url', '')\n",
    "        \n",
    "        full_content = f\"{title} {text}\".strip()\n",
    "        \n",
    "        # Check if story is OpenAI-related\n",
    "        if not contains_openai_keywords(full_content):\n",
    "            return None\n",
    "            \n",
    "        return {\n",
    "            'id': story_item['id'],\n",
    "            'title': clean_text(title),\n",
    "            'text': clean_text(text),\n",
    "            'url': url,\n",
    "            'score': story_item.get('score', 0),\n",
    "            'descendants': story_item.get('descendants', 0),  # comment count\n",
    "            'time': story_item.get('time', 0),\n",
    "            'created_date': datetime.fromtimestamp(story_item.get('time', 0)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'author': story_item.get('by', 'unknown'),\n",
    "            'content_type': 'story',\n",
    "            'full_text': clean_text(full_content),\n",
    "            'text_length': len(clean_text(full_content)),\n",
    "            'kids': story_item.get('kids', [])  # comment IDs\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting story data: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_comment_data(comment_item: Dict, story_id: int, depth: int = 0) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"Extract relevant data from a HN comment\"\"\"\n",
    "    try:\n",
    "        if not comment_item or comment_item.get('deleted') or comment_item.get('dead'):\n",
    "            return None\n",
    "            \n",
    "        text = comment_item.get('text', '')\n",
    "        if not text or len(text) < COLLECTION_PARAMS['min_comment_length']:\n",
    "            return None\n",
    "            \n",
    "        clean_comment_text = clean_text(text)\n",
    "        if len(clean_comment_text) < COLLECTION_PARAMS['min_comment_length']:\n",
    "            return None\n",
    "            \n",
    "        return {\n",
    "            'id': comment_item['id'],\n",
    "            'parent_id': comment_item.get('parent', story_id),\n",
    "            'story_id': story_id,\n",
    "            'text': clean_comment_text,\n",
    "            'time': comment_item.get('time', 0),\n",
    "            'created_date': datetime.fromtimestamp(comment_item.get('time', 0)).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'author': comment_item.get('by', 'unknown'),\n",
    "            'content_type': 'comment',\n",
    "            'full_text': clean_comment_text,\n",
    "            'text_length': len(clean_comment_text),\n",
    "            'depth': depth,\n",
    "            'kids': comment_item.get('kids', [])  # reply IDs\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting comment data: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"üìù Data processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41767dfb-a5f1-420a-9022-93c27550008b",
   "metadata": {},
   "source": [
    "## Story Collection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc0dd6-2965-4361-818e-5bbb76ba7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_relevant_stories() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Collect OpenAI-related stories from HN\"\"\"\n",
    "    print(\"üì± Collecting OpenAI-related stories from Hacker News...\")\n",
    "    \n",
    "    collected_stories = []\n",
    "    processed_count = 0\n",
    "    relevant_count = 0\n",
    "    \n",
    "    # Get story IDs from multiple sources\n",
    "    print(\"üîç Fetching story lists...\")\n",
    "    top_stories = hn_api.get_top_stories(limit=100)\n",
    "    new_stories = hn_api.get_new_stories(limit=100)\n",
    "    best_stories = hn_api.get_best_stories(limit=50)\n",
    "    \n",
    "    # Combine and deduplicate\n",
    "    all_story_ids = list(set(top_stories + new_stories + best_stories))\n",
    "    print(f\"   Found {len(all_story_ids)} unique stories to check\")\n",
    "    \n",
    "    # Calculate time cutoff\n",
    "    cutoff_time = time.time() - (COLLECTION_PARAMS['stories_lookback_hours'] * 3600)\n",
    "    \n",
    "    for story_id in all_story_ids[:COLLECTION_PARAMS['max_stories']]:\n",
    "        processed_count += 1\n",
    "        \n",
    "        if processed_count % 20 == 0:\n",
    "            print(f\"   üìä Processed {processed_count}/{len(all_story_ids)} stories, found {relevant_count} relevant\")\n",
    "        \n",
    "        # Get story details\n",
    "        story_item = hn_api.get_item(story_id)\n",
    "        if not story_item:\n",
    "            continue\n",
    "            \n",
    "        # Check if story is recent enough\n",
    "        if story_item.get('time', 0) < cutoff_time:\n",
    "            continue\n",
    "            \n",
    "        # Extract and check relevance\n",
    "        story_data = extract_story_data(story_item)\n",
    "        if story_data:\n",
    "            collected_stories.append(story_data)\n",
    "            relevant_count += 1\n",
    "            print(f\"   ‚úÖ Found: '{story_data['title'][:60]}...' (Score: {story_data['score']})\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    print(f\"\\nüìà Story collection complete:\")\n",
    "    print(f\"   Processed: {processed_count} stories\")\n",
    "    print(f\"   Relevant: {relevant_count} OpenAI-related stories\")\n",
    "    \n",
    "    return collected_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f6544-8013-4644-ae91-2cf29c29c5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect stories\n",
    "relevant_stories = collect_relevant_stories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf23fd5-897c-4692-9d4a-8ac3a7371517",
   "metadata": {},
   "source": [
    "## Comment Collection Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7d1ebe-5d28-4ff5-b4b6-94597bc8a9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_comments_for_story(story_data: Dict, max_comments: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Collect comments for a specific story\"\"\"\n",
    "    collected_comments = []\n",
    "    \n",
    "    if not story_data.get('kids'):\n",
    "        return collected_comments\n",
    "    \n",
    "    comment_queue = [(kid_id, 0) for kid_id in story_data['kids'][:max_comments]]\n",
    "    \n",
    "    while comment_queue and len(collected_comments) < max_comments:\n",
    "        comment_id, depth = comment_queue.pop(0)\n",
    "        \n",
    "        if depth > COLLECTION_PARAMS['max_comment_depth']:\n",
    "            continue\n",
    "            \n",
    "        # Get comment details\n",
    "        comment_item = hn_api.get_item(comment_id)\n",
    "        if not comment_item:\n",
    "            continue\n",
    "            \n",
    "        # Extract comment data\n",
    "        comment_data = extract_comment_data(comment_item, story_data['id'], depth)\n",
    "        if comment_data:\n",
    "            collected_comments.append(comment_data)\n",
    "            \n",
    "            # Add replies to queue if we haven't gone too deep\n",
    "            if depth < COLLECTION_PARAMS['max_comment_depth'] and comment_item.get('kids'):\n",
    "                for kid_id in comment_item['kids'][:5]:  # Limit replies per comment\n",
    "                    comment_queue.append((kid_id, depth + 1))\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    return collected_comments\n",
    "\n",
    "def collect_all_comments(stories: List[Dict]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Collect comments for all relevant stories\"\"\"\n",
    "    print(\"üí¨ Collecting comments for relevant stories...\")\n",
    "    \n",
    "    all_comments = []\n",
    "    \n",
    "    for i, story in enumerate(stories):\n",
    "        print(f\"   üìù Processing comments for story {i+1}/{len(stories)}: '{story['title'][:50]}...'\")\n",
    "        \n",
    "        comments = collect_comments_for_story(story, COLLECTION_PARAMS['max_comments_per_story'])\n",
    "        all_comments.extend(comments)\n",
    "        \n",
    "        print(f\"      Found {len(comments)} comments\")\n",
    "        \n",
    "        # Rate limiting between stories\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    print(f\"\\nüí¨ Comment collection complete: {len(all_comments)} comments collected\")\n",
    "    return all_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9afa15-3b5e-4d30-bc24-d4fb399cefb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect comments for all relevant stories\n",
    "if relevant_stories:\n",
    "    collected_comments = collect_all_comments(relevant_stories)\n",
    "else:\n",
    "    collected_comments = []\n",
    "    print(\"‚ö†Ô∏è No relevant stories found, skipping comment collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc379b08-4e3a-4808-89ad-189afc88ce0c",
   "metadata": {},
   "source": [
    "## Combine and Analyze Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0033b6-6ae9-4cd2-9a2c-d038fb63e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all collected data\n",
    "all_collected_data = relevant_stories + collected_comments\n",
    "\n",
    "print(\"üìä Data Analysis:\")\n",
    "print(f\"   Total items collected: {len(all_collected_data)}\")\n",
    "print(f\"   Stories: {len(relevant_stories)}\")\n",
    "print(f\"   Comments: {len(collected_comments)}\")\n",
    "\n",
    "if all_collected_data:\n",
    "    # Convert to DataFrame for analysis\n",
    "    df = pd.DataFrame(all_collected_data)\n",
    "    \n",
    "    print(f\"   Date range: {df['created_date'].min()} to {df['created_date'].max()}\")\n",
    "    print(f\"   Average text length: {df['text_length'].mean():.0f} characters\")\n",
    "    \n",
    "    # Show top stories by score\n",
    "    story_df = df[df['content_type'] == 'story'].sort_values('score', ascending=False)\n",
    "    print(f\"\\nüî• Top stories by score:\")\n",
    "    for idx, story in story_df.head(5).iterrows():\n",
    "        print(f\"   ‚Ä¢ {story['title'][:70]}... (Score: {story['score']}, Comments: {story['descendants']})\")\n",
    "    \n",
    "    # Show comment distribution\n",
    "    if len(collected_comments) > 0:\n",
    "        comment_df = df[df['content_type'] == 'comment']\n",
    "        print(f\"\\nüí¨ Comment insights:\")\n",
    "        print(f\"   Average comment length: {comment_df['text_length'].mean():.0f} characters\")\n",
    "        print(f\"   Comment depth distribution: {comment_df['depth'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Show sample content\n",
    "    print(f\"\\nüîç Sample collected content:\")\n",
    "    for idx, item in df.head(3).iterrows():\n",
    "        content_preview = item['full_text'][:100] + \"...\" if len(item['full_text']) > 100 else item['full_text']\n",
    "        print(f\"   [{item['content_type']}] {content_preview}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data collected. Consider adjusting keywords or time range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615fa9e3-e7ac-47fd-8eac-7d55e0eda0a3",
   "metadata": {},
   "source": [
    "## GCP Upload Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2fcf94-b615-4e7d-9b20-87948593fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCPUploader:\n",
    "    \"\"\"Handles uploading data to Google Cloud Storage\"\"\"\n",
    "    \n",
    "    def __init__(self, gcp_config: dict):\n",
    "        if gcp_config.get('credentials_path'):\n",
    "            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = gcp_config['credentials_path']\n",
    "        \n",
    "        self.client = storage.Client(project=gcp_config['project_id'])\n",
    "        self.bucket_name = gcp_config['bucket_name']\n",
    "        self.bucket = self.client.bucket(gcp_config['bucket_name'])\n",
    "        \n",
    "        print(f\"‚òÅÔ∏è Connected to GCS bucket: {self.bucket_name}\")\n",
    "    \n",
    "    def upload_json_data(self, data: List[Dict[str, Any]], filename: str = None) -> str:\n",
    "        \"\"\"Upload collected data as JSON to GCS bucket\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"hackernews_openai_data_{timestamp}.json\"\n",
    "        \n",
    "        # Convert to JSON\n",
    "        json_data = json.dumps(data, indent=2, default=str)\n",
    "        \n",
    "        try:\n",
    "            # Upload to bucket\n",
    "            blob = self.bucket.blob(f\"raw_data/{filename}\")\n",
    "            blob.upload_from_string(json_data, content_type='application/json')\n",
    "            \n",
    "            gcs_uri = f\"gs://{self.bucket_name}/raw_data/{filename}\"\n",
    "            print(f\"‚úÖ JSON data uploaded to: {gcs_uri}\")\n",
    "            \n",
    "            return gcs_uri\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to upload JSON data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def upload_csv_data(self, data: List[Dict[str, Any]], filename: str = None) -> str:\n",
    "        \"\"\"Convert to DataFrame and upload as CSV\"\"\"\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"hackernews_openai_data_{timestamp}.csv\"\n",
    "        \n",
    "        try:\n",
    "            # Upload CSV\n",
    "            csv_data = df.to_csv(index=False)\n",
    "            blob = self.bucket.blob(f\"processed_data/{filename}\")\n",
    "            blob.upload_from_string(csv_data, content_type='text/csv')\n",
    "            \n",
    "            gcs_uri = f\"gs://{self.bucket_name}/processed_data/{filename}\"\n",
    "            print(f\"‚úÖ CSV data uploaded to: {gcs_uri}\")\n",
    "            \n",
    "            return gcs_uri\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to upload CSV data: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def upload_metadata(self, data_stats: dict, filename: str = None) -> str:\n",
    "        \"\"\"Upload collection metadata\"\"\"\n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"hn_metadata_{timestamp}.json\"\n",
    "        \n",
    "        metadata = {\n",
    "            'collection_timestamp': datetime.now().isoformat(),\n",
    "            'data_source': 'hacker_news',\n",
    "            'collection_params': COLLECTION_PARAMS,\n",
    "            'openai_keywords': OPENAI_KEYWORDS,\n",
    "            **data_stats\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            metadata_blob = self.bucket.blob(f\"metadata/{filename}\")\n",
    "            metadata_blob.upload_from_string(\n",
    "                json.dumps(metadata, indent=2, default=str), \n",
    "                content_type='application/json'\n",
    "            )\n",
    "            \n",
    "            gcs_uri = f\"gs://{self.bucket_name}/metadata/{filename}\"\n",
    "            print(f\"‚úÖ Metadata uploaded to: {gcs_uri}\")\n",
    "            return gcs_uri\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to upload metadata: {e}\")\n",
    "            raise\n",
    "\n",
    "print(\"‚òÅÔ∏è GCP Uploader class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db776225-32c8-47dd-a6b9-3354e5562121",
   "metadata": {},
   "source": [
    "## Upload to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64fd447-036a-46fe-a53a-133cdb65910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_collected_data:\n",
    "    # Initialize uploader\n",
    "    uploader = GCPUploader(GCP_CONFIG)\n",
    "    \n",
    "    # Prepare data statistics\n",
    "    data_stats = {\n",
    "        'total_records': len(all_collected_data),\n",
    "        'stories_count': len(relevant_stories),\n",
    "        'comments_count': len(collected_comments),\n",
    "        'date_range_start': df['created_date'].min() if not df.empty else None,\n",
    "        'date_range_end': df['created_date'].max() if not df.empty else None,\n",
    "        'avg_score': float(story_df['score'].mean()) if not story_df.empty else 0,\n",
    "        'avg_text_length': float(df['text_length'].mean()) if not df.empty else 0,\n",
    "        'total_story_score': int(story_df['score'].sum()) if not story_df.empty else 0\n",
    "    }\n",
    "    \n",
    "    print(\"üì§ Uploading data to GCS...\")\n",
    "    \n",
    "    try:\n",
    "        # Upload JSON data\n",
    "        json_uri = uploader.upload_json_data(all_collected_data)\n",
    "        \n",
    "        # Upload CSV data  \n",
    "        csv_uri = uploader.upload_csv_data(all_collected_data)\n",
    "        \n",
    "        # Upload metadata\n",
    "        metadata_uri = uploader.upload_metadata(data_stats)\n",
    "        \n",
    "        print(f\"\\nüéâ Upload completed successfully!\")\n",
    "        print(f\"   üìÅ JSON: {json_uri}\")\n",
    "        print(f\"   üìä CSV: {csv_uri}\")\n",
    "        print(f\"   üìã Metadata: {metadata_uri}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Upload failed: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data to upload. Try adjusting collection parameters or keywords.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a50ef8-2799-4d68-a8be-9aa1dbe0087c",
   "metadata": {},
   "source": [
    "## Final Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba432dd-45e4-4d23-8402-5553c7ed3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"üöÄ HACKER NEWS DATA COLLECTION COMPLETED!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if all_collected_data:\n",
    "    print(f\"\\nüìä Final Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total OpenAI-related items: {len(all_collected_data)}\")\n",
    "    print(f\"   ‚Ä¢ Stories: {len(relevant_stories)}\")\n",
    "    print(f\"   ‚Ä¢ Comments: {len(collected_comments)}\")\n",
    "    print(f\"   ‚Ä¢ Average story score: {story_df['score'].mean():.1f}\" if not story_df.empty else \"   ‚Ä¢ No stories collected\")\n",
    "    print(f\"   ‚Ä¢ Total engagement: {story_df['descendants'].sum()} comments across all stories\" if not story_df.empty else \"\")\n",
    "    print(f\"   ‚Ä¢ Average text length: {df['text_length'].mean():.0f} characters\")\n",
    "    \n",
    "    print(f\"\\nüéØ Data Quality Insights:\")\n",
    "    print(f\"   ‚Ä¢ High-quality tech discourse from HN community\")\n",
    "    print(f\"   ‚Ä¢ Mix of technical and business perspectives\")\n",
    "    print(f\"   ‚Ä¢ Recent discussions (last {COLLECTION_PARAMS['stories_lookback_hours']} hours)\")\n",
    "    print(f\"   ‚Ä¢ Engaged community (average story score: {story_df['score'].mean():.1f})\")\n",
    "    \n",
    "    print(f\"\\nüîÆ Next Steps:\")\n",
    "    print(f\"   1. ‚úÖ High-quality HN data collected and stored in GCS\")\n",
    "    print(f\"   2. üè∑Ô∏è  Create sentiment labeling pipeline (HN tends to be more analytical)\")\n",
    "    print(f\"   3. üîß Preprocess data for BERT fine-tuning\")\n",
    "    print(f\"   4. üéØ Fine-tune your existing BERT model on this tech-focused data\")\n",
    "    print(f\"   5. üìà Compare performance: movie reviews ‚Üí HN tech discourse\")\n",
    "    \n",
    "    print(f\"\\nüí° Pro Tips:\")\n",
    "    print(f\"   ‚Ä¢ HN discussions are often more nuanced than binary sentiment\")\n",
    "    print(f\"   ‚Ä¢ Consider multi-class labels: positive/negative/neutral/analytical\")\n",
    "    print(f\"   ‚Ä¢ Tech jargon and acronyms are common - great for domain adaptation\")\n",
    "    print(f\"   ‚Ä¢ High signal-to-noise ratio compared to other social platforms\")\n",
    "    \n",
    "    # Show sample of highest-scoring content\n",
    "    if not story_df.empty:\n",
    "        print(f\"\\nüî• Top collected stories:\")\n",
    "        for idx, story in story_df.head(3).iterrows():\n",
    "            print(f\"   ‚Ä¢ [{story['score']} pts] {story['title']}\")\n",
    "            print(f\"     {story['descendants']} comments | {story['created_date']}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è No data collected this run. Consider:\")\n",
    "    print(f\"   ‚Ä¢ Expanding keywords list\")\n",
    "    print(f\"   ‚Ä¢ Increasing lookback time window\")\n",
    "    print(f\"   ‚Ä¢ Checking if there are recent OpenAI discussions on HN\")\n",
    "\n",
    "print(f\"\\nüéä Ready for the next phase: Transform this high-quality tech discourse into training data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7da77e9-dcd7-47a7-bace-7b6337fc1f59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa1a9b-52f7-4e89-88f4-dc86c798ef8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ce068a-9ce9-418a-91e1-25fa551f3e20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
