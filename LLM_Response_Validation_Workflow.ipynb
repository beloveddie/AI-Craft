{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNmwYwzrtViKmBOIxUUd9K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/beloveddie/AI-Craft/blob/main/LLM_Response_Validation_Workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be0OPm7Fn5WO",
        "outputId": "08ad318d-17d2-4a35-819b-34220c5df809"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.6/263.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get OPENAI_API_KEY_SOLID and set as OPENAI_API_KEY env variable\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_KEY_SOLID')"
      ],
      "metadata": {
        "id": "ktfOxcAaoioo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.workflow import (\n",
        "    Event,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step\n",
        ")\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "\n",
        "# Output Validation Workflow - Customer Support Focus\n",
        "class ResponseEvent(Event):\n",
        "    query: str\n",
        "    response: str\n",
        "\n",
        "class ValidatedResponseEvent(Event):\n",
        "    response: str\n",
        "    validation_result: Dict[str, Any]\n",
        "\n",
        "class ResponseValidationFlow(Workflow):\n",
        "    llm = OpenAI(model=\"gpt-4.1-mini\")\n",
        "\n",
        "    @step\n",
        "    async def generate_response(self, ev: StartEvent) -> ResponseEvent:\n",
        "        query = ev.query\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Act as a customer support assistant and respond to the following query:\n",
        "\n",
        "        QUERY: \"{query}\"\n",
        "\n",
        "        Provide a helpful, friendly response that addresses the query directly.\n",
        "        \"\"\"\n",
        "\n",
        "        response = await self.llm.acomplete(prompt)\n",
        "        return ResponseEvent(query=query, response=str(response))\n",
        "\n",
        "    @step\n",
        "    async def validate_response(self, ev: ResponseEvent) -> ValidatedResponseEvent:\n",
        "        response = ev.response\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Validate the following customer support response based on our brand guidelines:\n",
        "\n",
        "        RESPONSE: \"{response}\"\n",
        "\n",
        "        Analyze the response for:\n",
        "        1. Tone alignment (friendly, professional, empathetic)\n",
        "        2. Brand voice consistency\n",
        "        3. Solution completeness\n",
        "        4. Policy compliance\n",
        "        5. Proper disclaimers where needed\n",
        "        6. Absence of incorrect/misleading information\n",
        "\n",
        "        Return a JSON object with:\n",
        "        {{\n",
        "            \"compliance_score\": float (0-1),\n",
        "            \"tone_score\": float (0-1),\n",
        "            \"completeness_score\": float (0-1),\n",
        "            \"issues\": [specific issues found],\n",
        "            \"suggestions\": [improvement recommendations]\n",
        "        }}\n",
        "\n",
        "        Without the JSON formatting syntax.\n",
        "        \"\"\"\n",
        "\n",
        "        validation_result = await self.llm.acomplete(prompt)\n",
        "        return ValidatedResponseEvent(\n",
        "            response=response,\n",
        "            validation_result=dict(validation_result)\n",
        "        )\n",
        "\n",
        "    @step\n",
        "    async def finalize_response(self, ev: ValidatedResponseEvent) -> StopEvent:\n",
        "        validation_result = ev.validation_result\n",
        "\n",
        "        # In a real implementation, we would parse the validation result\n",
        "        # and potentially modify the response or reject it if it doesn't meet standards\n",
        "        prompt = f\"\"\"\n",
        "        Based on this validation result:\n",
        "        {validation_result}\n",
        "\n",
        "        Determine if this response meets our quality standards (compliance_score > 0.8).\n",
        "        If not, revise the response to address the issues while maintaining the core message.\n",
        "\n",
        "        Return a JSON object with:\n",
        "        {{\n",
        "            \"revised_response\": str,\n",
        "            \"complaince_score\": float (0-1),\n",
        "            \"completeness_score\": float (0-1),\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        final_response = await self.llm.acomplete(prompt)\n",
        "\n",
        "        return StopEvent(result={\n",
        "            \"assistant_response\": ev.response,\n",
        "            \"final_validation_response\": str(final_response)\n",
        "        })"
      ],
      "metadata": {
        "id": "XgUc6DQRoMYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Response Validation Example\n",
        "import json\n",
        "response_flow = ResponseValidationFlow(timeout=60, verbose=False)\n",
        "response_result = await response_flow.run(query=\"I've been waiting for my refund for 2 weeks now. This is unacceptable!\")\n",
        "print(json.dumps(response_result, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OP8mYZsokGQ",
        "outputId": "c6e77ca5-4bb6-48f3-92d3-5d4bb0aa427f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"assistant_response\": \"Hello! I\\u2019m really sorry to hear that your refund has been delayed and understand how frustrating this must be. Let me look into your order right away to see what\\u2019s causing the delay. Could you please provide me with your order number or any related details? We\\u2019ll do our best to resolve this for you as quickly as possible. Thank you for your patience!\",\n",
            "  \"final_validation_response\": \"{\\n  \\\"revised_response\\\": \\\"Thank you for reaching out regarding your refund. Typically, our refund processing takes 5-7 business days. For more details, you can visit our refund policy page on our website. To assist you further, could you please provide your order number and the email address used for the purchase? Rest assured, your information will be handled confidentially and used solely for this inquiry. Please note that external factors, such as payment processor delays, may occasionally affect refund timelines. Once I have your details, I will review your order and get back to you within 24 hours with an update.\\\",\\n  \\\"complaince_score\\\": 0.95,\\n  \\\"completeness_score\\\": 0.95\\n}\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(response_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu6v4mmxowoC",
        "outputId": "ec29ebbf-5dd3-442d-c007-f4161df816b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_result.get('assistant_response'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60grSFQ-znzd",
        "outputId": "cb5a4aa3-f5b8-49d2-def6-07eef94e2504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! I’m really sorry to hear that your refund has been delayed and understand how frustrating this must be. Let me look into your order right away to see what’s causing the delay. Could you please provide me with your order number or any related details? We’ll do our best to resolve this for you as quickly as possible. Thank you for your patience!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response_result.get('final_validation_response'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOtwV2BPz0vY",
        "outputId": "e0ed1c9f-8af3-41b8-9460-95108c075034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"revised_response\": \"Thank you for reaching out regarding your refund. Typically, our refund processing takes 5-7 business days. For more details, you can visit our refund policy page on our website. To assist you further, could you please provide your order number and the email address used for the purchase? Rest assured, your information will be handled confidentially and used solely for this inquiry. Please note that external factors, such as payment processor delays, may occasionally affect refund timelines. Once I have your details, I will review your order and get back to you within 24 hours with an update.\",\n",
            "  \"complaince_score\": 0.95,\n",
            "  \"completeness_score\": 0.95\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ekuVvTq2z4k2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}