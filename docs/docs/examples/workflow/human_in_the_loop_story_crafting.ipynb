{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "772ebb07-8c83-4e20-97d4-8d68ef2d642f",
      "metadata": {
        "id": "772ebb07-8c83-4e20-97d4-8d68ef2d642f"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/workflow/human_in_the_loop_story_crafting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4a4fc96-58a8-4b0b-a4fb-077ef16b59e2",
      "metadata": {
        "id": "b4a4fc96-58a8-4b0b-a4fb-077ef16b59e2"
      },
      "source": [
        "# Choose Your Own Adventure Workflow (Human In The Loop)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e631036a-1a21-4f5b-a497-b95f8a9438f5",
      "metadata": {
        "id": "e631036a-1a21-4f5b-a497-b95f8a9438f5"
      },
      "source": [
        "For some Workflow applications, it may desirable and/or required to have humans involved in its execution. For example, a step of a Workflow may need human expertise or input in order to run. In another scenario, it may be required to have a human validate the initial output of a Workflow.\n",
        "\n",
        "In this notebook, we show how one can implement a human-in-the-loop pattern with Workflows. Here we'll build a Workflow that creates stories in the style of Choose Your Own Adventure, where the LLM produces a segment of the story along with potential actions, and a human is required to choose from one of those actions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b69b368-c7f7-4aee-9e15-4fcd325e238a",
      "metadata": {
        "id": "9b69b368-c7f7-4aee-9e15-4fcd325e238a"
      },
      "source": [
        "## Generating Segments Of The Story With An LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f093ac20-386d-4eac-86c5-0d4134a77099",
      "metadata": {
        "id": "f093ac20-386d-4eac-86c5-0d4134a77099"
      },
      "source": [
        "Here, we'll make use of the ability to produce structured outputs from an LLM. We will task the LLM to create a segment of the story that is in continuation of previously generated segments and action choices."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install the required packages\n",
        "!pip install llama-index"
      ],
      "metadata": {
        "id": "MVORFrQvXKmF",
        "outputId": "642c92af-3ea9-4ee3-84b2-264dedc50f83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MVORFrQvXKmF",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.29-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl.metadata (727 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.1 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.29 (from llama-index)\n",
            "  Downloading llama_index_core-0.12.29-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.32-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.70.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.29->llama-index) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (3.11.15)\n",
            "Collecting banks<3.0.0,>=2.0.0 (from llama-index-core<0.13.0,>=0.12.29->llama-index)\n",
            "  Downloading banks-2.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.29->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (1.2.18)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.29->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.29->llama-index)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (2025.3.2)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (2.11.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (9.1.2)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.29->llama-index)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (4.13.1)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.29->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.29->llama-index) (1.17.2)\n",
            "Collecting llama-cloud<0.2.0,>=0.1.13 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.18-py3-none-any.whl.metadata (902 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.2.2)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.6.4.post1-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.29->llama-index) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.29->llama-index) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.29->llama-index) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.29->llama-index) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.29->llama-index) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.29->llama-index) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.29->llama-index) (1.18.3)\n",
            "Collecting griffe (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.29->llama-index)\n",
            "  Downloading griffe-1.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.29->llama-index) (3.1.6)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.29->llama-index) (4.3.7)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.11/dist-packages (from llama-cloud<0.2.0,>=0.1.13->llama-index-indices-managed-llama-cloud>=0.4.0->llama-index) (2025.1.31)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.29->llama-index) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.29->llama-index) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.29->llama-index) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.29->llama-index) (0.14.0)\n",
            "Collecting llama-cloud-services>=0.6.4 (from llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud_services-0.6.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.29->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.29->llama-index) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.29->llama-index) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.29->llama-index) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.29->llama-index) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.29->llama-index) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.29->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.29->llama-index)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2025.2)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from llama-cloud-services>=0.6.4->llama-parse>=0.5.0->llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.29->llama-index) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (1.17.0)\n",
            "Collecting colorama>=0.4 (from griffe->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.29->llama-index)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3.0.0,>=2.0.0->llama-index-core<0.13.0,>=0.12.29->llama-index) (3.0.2)\n",
            "Downloading llama_index-0.12.29-py3-none-any.whl (7.0 kB)\n",
            "Downloading llama_index_agent_openai-0.4.6-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.1-py3-none-any.whl (28 kB)\n",
            "Downloading llama_index_core-0.12.29-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.11-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_llms_openai-0.3.32-py3-none-any.whl (23 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.3-py3-none-any.whl (5.9 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.7-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading banks-2.1.1-py3-none-any.whl (28 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.18-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.6.4.post1-py3-none-any.whl (4.9 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading llama_cloud_services-0.6.9-py3-none-any.whl (29 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading griffe-1.7.2-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, python-dotenv, pypdf, mypy-extensions, marshmallow, colorama, typing-inspect, tiktoken, griffe, llama-cloud, dataclasses-json, banks, llama-index-core, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-cloud-services, llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-readers-llama-parse, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed banks-2.1.1 colorama-0.4.6 dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.7.2 llama-cloud-0.1.18 llama-cloud-services-0.6.9 llama-index-0.12.29 llama-index-agent-openai-0.4.6 llama-index-cli-0.4.1 llama-index-core-0.12.29 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.11 llama-index-llms-openai-0.3.32 llama-index-multi-modal-llms-openai-0.4.3 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.7 llama-index-readers-llama-parse-0.4.0 llama-parse-0.6.4.post1 marshmallow-3.26.1 mypy-extensions-1.0.0 pypdf-5.4.0 python-dotenv-1.1.0 striprtf-0.0.26 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e389bf9f-3048-4372-b967-9aa909990eef",
      "metadata": {
        "id": "e389bf9f-3048-4372-b967-9aa909990eef"
      },
      "outputs": [],
      "source": [
        "from typing import Any, List\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.bridge.pydantic import BaseModel, Field\n",
        "from llama_index.core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d42111a0-cb06-47f1-af60-e4e4628e8c16",
      "metadata": {
        "id": "d42111a0-cb06-47f1-af60-e4e4628e8c16"
      },
      "outputs": [],
      "source": [
        "class Segment(BaseModel):\n",
        "    \"\"\"Data model for generating segments of a story.\"\"\"\n",
        "\n",
        "    plot: str = Field(\n",
        "        description=\"The plot of the adventure for the current segment. The plot should be no longer than 3 sentences.\"\n",
        "    )\n",
        "    actions: List[str] = Field(\n",
        "        default=[],\n",
        "        description=\"The list of actions the protaganist can take that will shape the plot and actions of the next segment.\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6826965f-58eb-4027-a431-069734ed671f",
      "metadata": {
        "id": "6826965f-58eb-4027-a431-069734ed671f"
      },
      "outputs": [],
      "source": [
        "SEGMENT_GENERATION_TEMPLATE = \"\"\"\n",
        "You are working with a human to create a story in the style of choose your own adventure.\n",
        "\n",
        "The human is playing the role of the protaganist in the story which you are tasked to\n",
        "help write. To create the story, we do it in steps, where each step produces a BLOCK.\n",
        "Each BLOCK consists of a PLOT, a set of ACTIONS that the protaganist can take, and the\n",
        "chosen ACTION.\n",
        "\n",
        "Below we attach the history of the adventure so far.\n",
        "\n",
        "PREVIOUS BLOCKS:\n",
        "---\n",
        "{running_story}\n",
        "\n",
        "Continue the story by generating the next block's PLOT and set of ACTIONs. If there are\n",
        "no previous BLOCKs, start an interesting brand new story. Give the protaganist a name and an\n",
        "interesting challenge to solve.\n",
        "\n",
        "\n",
        "Use the provided data model to structure your output.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "758b5b54-c342-4fd0-b1b4-a58a54707a86",
      "metadata": {
        "id": "758b5b54-c342-4fd0-b1b4-a58a54707a86"
      },
      "outputs": [],
      "source": [
        "FINAL_SEGMENT_GENERATION_TEMPLATE = \"\"\"\n",
        "You are working with a human to create a story in the style of choose your own adventure.\n",
        "\n",
        "The human is playing the role of the protaganist in the story which you are tasked to\n",
        "help write. To create the story, we do it in steps, where each step produces a BLOCK.\n",
        "Each BLOCK consists of a PLOT, a set of ACTIONS that the protaganist can take, and the\n",
        "chosen ACTION. Below we attach the history of the adventure so far.\n",
        "\n",
        "PREVIOUS BLOCKS:\n",
        "---\n",
        "{running_story}\n",
        "\n",
        "The story is now coming to an end. With the previous blocks, wrap up the story with a\n",
        "closing PLOT. Since it is a closing plot, DO NOT GENERATE a new set of actions.\n",
        "\n",
        "Use the provided data model to structure your output.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set OPENAI_API_KEY from Colab Secrets\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "4YPsVDIIYI0A"
      },
      "id": "4YPsVDIIYI0A",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7f92da9d-e2e2-4998-a019-bb95e36c9f3c",
      "metadata": {
        "id": "7f92da9d-e2e2-4998-a019-bb95e36c9f3c"
      },
      "outputs": [],
      "source": [
        "# Let's see an example segment\n",
        "llm = OpenAI(\"gpt-4o-mini\")\n",
        "segment = llm.structured_predict(\n",
        "    Segment,\n",
        "    PromptTemplate(SEGMENT_GENERATION_TEMPLATE),\n",
        "    running_story=\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3c51333c-d95f-4127-97ae-e69178988ff2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c51333c-d95f-4127-97ae-e69178988ff2",
        "outputId": "a1fe46dd-5aaa-4d91-99be-7dabe8030fd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Segment(plot='In the heart of the enchanted forest, Elara, a skilled herbalist, discovers a hidden glade filled with glowing flowers. As she approaches, she hears whispers that seem to come from the flowers themselves, revealing a secret about a lost treasure hidden deep within the forest. However, a dark shadow looms nearby, threatening to guard the treasure and keep it hidden forever.', actions=['Investigate the glowing flowers', 'Follow the whispers deeper into the forest', 'Set a trap for the shadow', 'Leave the glade and return home'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "segment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1408ab3-3764-415a-b70c-d2fca69a2dac",
      "metadata": {
        "id": "e1408ab3-3764-415a-b70c-d2fca69a2dac"
      },
      "source": [
        "### Stitching together previous segments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfbdad71-ff46-4044-b2a6-68721f32df81",
      "metadata": {
        "id": "cfbdad71-ff46-4044-b2a6-68721f32df81"
      },
      "source": [
        "We need to stich together story segments and pass this in to the prompt as the value for `running_story`. We define a `Block` data class that holds the `Segment` as well as the `choice` of action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "19c4ed39-a2af-4c89-9dba-ddd49aaa0cd0",
      "metadata": {
        "id": "19c4ed39-a2af-4c89-9dba-ddd49aaa0cd0"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "from typing import Optional\n",
        "\n",
        "BLOCK_TEMPLATE = \"\"\"\n",
        "BLOCK\n",
        "===\n",
        "PLOT: {plot}\n",
        "ACTIONS: {actions}\n",
        "CHOICE: {choice}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Block(BaseModel):\n",
        "    id_: str = Field(default_factory=lambda: str(uuid.uuid4()))\n",
        "    segment: Segment\n",
        "    choice: Optional[str] = None\n",
        "    block_template: str = BLOCK_TEMPLATE\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.block_template.format(\n",
        "            plot=self.segment.plot,\n",
        "            actions=\", \".join(self.segment.actions),\n",
        "            choice=self.choice or \"\",\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dda2aed4-9df1-4d00-813c-8298b39c5e85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dda2aed4-9df1-4d00-813c-8298b39c5e85",
        "outputId": "cc6a22d0-b38b-4671-f4f2-9fd39968889c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLOCK\n",
            "===\n",
            "PLOT: In the heart of the enchanted forest, Elara, a skilled herbalist, discovers a hidden glade filled with glowing flowers. As she approaches, she hears whispers that seem to come from the flowers themselves, revealing a secret about a lost treasure hidden deep within the forest. However, a dark shadow looms nearby, threatening to guard the treasure and keep it hidden forever.\n",
            "ACTIONS: Investigate the glowing flowers, Follow the whispers deeper into the forest, Set a trap for the shadow, Leave the glade and return home\n",
            "CHOICE: \n",
            "\n"
          ]
        }
      ],
      "source": [
        "block = Block(segment=segment)\n",
        "print(block)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c103bff-342c-4b88-9f34-01489ec2dc76",
      "metadata": {
        "id": "8c103bff-342c-4b88-9f34-01489ec2dc76"
      },
      "source": [
        "## Create The Choose Your Own Adventure Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76ae34dd-9ce2-42d4-82ca-e6525fc163c3",
      "metadata": {
        "id": "76ae34dd-9ce2-42d4-82ca-e6525fc163c3"
      },
      "source": [
        "This Workflow will consist of two steps that will cycle until a max number of steps (i.e., segments) has been produced. The first step will have the LLM create a new `Segment`, which will be used to create a new story `Block`. The second step will prompt the human to choose their adventure from the list of actions specified in the newly created `Segment`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cf4d468a-f4de-478c-9f7b-8931d4dad4ef",
      "metadata": {
        "id": "cf4d468a-f4de-478c-9f7b-8931d4dad4ef"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.workflow import (\n",
        "    Context,\n",
        "    Event,\n",
        "    StartEvent,\n",
        "    StopEvent,\n",
        "    Workflow,\n",
        "    step,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "abed042e-afb7-4e29-9d23-1374d3583c30",
      "metadata": {
        "id": "abed042e-afb7-4e29-9d23-1374d3583c30"
      },
      "outputs": [],
      "source": [
        "class NewBlockEvent(Event):\n",
        "    block: Block\n",
        "\n",
        "\n",
        "class HumanChoiceEvent(Event):\n",
        "    block_id: str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "83ed549a-36ec-446c-963f-d0ced185622f",
      "metadata": {
        "id": "83ed549a-36ec-446c-963f-d0ced185622f"
      },
      "outputs": [],
      "source": [
        "class ChooseYourOwnAdventureWorkflow(Workflow):\n",
        "    def __init__(self, max_steps: int = 3, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.llm = OpenAI(\"gpt-4o-mini\")\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "    @step\n",
        "    async def create_segment(\n",
        "        self, ctx: Context, ev: StartEvent | HumanChoiceEvent\n",
        "    ) -> NewBlockEvent | StopEvent:\n",
        "        blocks = await ctx.get(\"blocks\", [])\n",
        "        running_story = \"\\n\".join(str(b) for b in blocks)\n",
        "\n",
        "        if len(blocks) < self.max_steps:\n",
        "            new_segment = self.llm.structured_predict(\n",
        "                Segment,\n",
        "                PromptTemplate(SEGMENT_GENERATION_TEMPLATE),\n",
        "                running_story=running_story,\n",
        "            )\n",
        "            new_block = Block(segment=new_segment)\n",
        "            blocks.append(new_block)\n",
        "            await ctx.set(\"blocks\", blocks)\n",
        "            return NewBlockEvent(block=new_block)\n",
        "        else:\n",
        "            final_segment = self.llm.structured_predict(\n",
        "                Segment,\n",
        "                PromptTemplate(FINAL_SEGMENT_GENERATION_TEMPLATE),\n",
        "                running_story=running_story,\n",
        "            )\n",
        "            final_block = Block(segment=final_segment)\n",
        "            blocks.append(final_block)\n",
        "            return StopEvent(result=blocks)\n",
        "\n",
        "    @step\n",
        "    async def prompt_human(\n",
        "        self, ctx: Context, ev: NewBlockEvent\n",
        "    ) -> HumanChoiceEvent:\n",
        "        block = ev.block\n",
        "\n",
        "        # get human input\n",
        "        human_prompt = f\"\\n===\\n{ev.block.segment.plot}\\n\\n\"\n",
        "        human_prompt += \"Choose your adventure:\\n\\n\"\n",
        "        human_prompt += \"\\n\".join(ev.block.segment.actions)\n",
        "        human_prompt += \"\\n\\n\"\n",
        "        human_input = input(human_prompt)\n",
        "\n",
        "        blocks = await ctx.get(\"blocks\")\n",
        "        block.choice = human_input\n",
        "        blocks[-1] = block\n",
        "        await ctx.set(\"block\", blocks)\n",
        "\n",
        "        return HumanChoiceEvent(block_id=ev.block.id_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "533cc3c8-d29a-4448-9237-a45855628d1e",
      "metadata": {
        "id": "533cc3c8-d29a-4448-9237-a45855628d1e"
      },
      "source": [
        "### Running The Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "431dc17c-16cc-4adb-a2d8-c9ee117e6480",
      "metadata": {
        "id": "431dc17c-16cc-4adb-a2d8-c9ee117e6480"
      },
      "source": [
        "Since workflows are async first, this all runs fine in a notebook. If you were running in your own code, you would want to use `asyncio.run()` to start an async event loop if one isn't already running.\n",
        "\n",
        "```python\n",
        "async def main():\n",
        "    <async code>\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import asyncio\n",
        "    asyncio.run(main())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "260e3f7b-a490-4708-b589-4e829a1e2580",
      "metadata": {
        "id": "260e3f7b-a490-4708-b589-4e829a1e2580"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "07fcdf83-b2ee-43ab-b2df-61931aca2888",
      "metadata": {
        "id": "07fcdf83-b2ee-43ab-b2df-61931aca2888"
      },
      "outputs": [],
      "source": [
        "w = ChooseYourOwnAdventureWorkflow(timeout=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "ba835938-d5cf-489f-b27a-0a0f76119cee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba835938-d5cf-489f-b27a-0a0f76119cee",
        "outputId": "0d1befab-fbdb-4263-f3a7-5a2a31027755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===\n",
            "In a small village nestled between towering mountains, a young woman named Elara discovers an ancient map hidden in her grandmother's attic. The map hints at a legendary treasure buried deep within the Forbidden Forest, a place rumored to be cursed. Driven by curiosity and the desire to change her family's fortunes, Elara decides to embark on a quest to find the treasure, but she must first gather supplies and seek advice from the village elder.\n",
            "\n",
            "Choose your adventure:\n",
            "\n",
            "Visit the village market to gather supplies\n",
            "Consult the village elder for advice\n",
            "Prepare a plan for the journey into the Forbidden Forest\n",
            "\n",
            "Consult the village elder for advice\n",
            "\n",
            "===\n",
            "Elara approaches the village elder, a wise woman named Maelis, who has lived for many years and knows the secrets of the Forbidden Forest. Maelis warns Elara about the dangers that lie ahead, including mythical creatures and treacherous terrain, but she also reveals that the treasure is protected by a riddle that must be solved. To aid Elara, Maelis offers her a magical amulet that can guide her through the forest, but warns that it can only be used once.\n",
            "\n",
            "Choose your adventure:\n",
            "\n",
            "Accept the magical amulet from Maelis\n",
            "Ask Maelis for more information about the riddle\n",
            "Inquire about the mythical creatures in the forest\n",
            "\n",
            "Ask Maelis for more information about the riddle\n",
            "\n",
            "===\n",
            "Maelis explains that the riddle is inscribed on a stone tablet located at the heart of the Forbidden Forest. She warns Elara that the riddle is known to confuse even the wisest of minds, and only those with a pure heart can decipher it. With this knowledge, Elara must now decide how to prepare for the challenges ahead, as the forest is filled with both wonders and dangers.\n",
            "\n",
            "Choose your adventure:\n",
            "\n",
            "Gather more information about the stone tablet\n",
            "Prepare her gear for the journey\n",
            "Seek out a companion to join her on the quest\n",
            "\n",
            "Maybe search our heart first....\n"
          ]
        }
      ],
      "source": [
        "result = await w.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c29727-1614-4a8c-b56b-69b5f9b553e1",
      "metadata": {
        "id": "d6c29727-1614-4a8c-b56b-69b5f9b553e1"
      },
      "source": [
        "### Print The Final Story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "1e15aa0c-78da-4ff6-9519-d0b134bdd546",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e15aa0c-78da-4ff6-9519-d0b134bdd546",
        "outputId": "5cd31db4-5652-41c7-a3c2-a8c6671108c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In a small village nestled between towering mountains, a young woman named Elara discovers an ancient map hidden in her grandmother's attic. The map hints at a legendary treasure buried deep within the Forbidden Forest, a place rumored to be cursed. Driven by curiosity and the desire to change her family's fortunes, Elara decides to embark on a quest to find the treasure, but she must first gather supplies and seek advice from the village elder.\n",
            "\n",
            "Elara approaches the village elder, a wise woman named Maelis, who has lived for many years and knows the secrets of the Forbidden Forest. Maelis warns Elara about the dangers that lie ahead, including mythical creatures and treacherous terrain, but she also reveals that the treasure is protected by a riddle that must be solved. To aid Elara, Maelis offers her a magical amulet that can guide her through the forest, but warns that it can only be used once.\n",
            "\n",
            "Maelis explains that the riddle is inscribed on a stone tablet located at the heart of the Forbidden Forest. She warns Elara that the riddle is known to confuse even the wisest of minds, and only those with a pure heart can decipher it. With this knowledge, Elara must now decide how to prepare for the challenges ahead, as the forest is filled with both wonders and dangers.\n",
            "\n",
            "Elara takes a moment to reflect on her intentions and the purity of her heart. With newfound clarity, she feels ready to face the challenges of the Forbidden Forest. Armed with the magical amulet and her unwavering spirit, she steps into the forest, ready to solve the riddle and uncover the legendary treasure, knowing that her journey is not just about riches, but about discovering her true self.\n"
          ]
        }
      ],
      "source": [
        "final_story = \"\\n\\n\".join(b.segment.plot for b in result)\n",
        "print(final_story)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce77433-8e18-4793-a7bb-604f5ba0c86c",
      "metadata": {
        "id": "3ce77433-8e18-4793-a7bb-604f5ba0c86c"
      },
      "source": [
        "### Other Ways To Implement Human In The Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20465b96-9382-4a4c-a2ec-767a092ba068",
      "metadata": {
        "id": "20465b96-9382-4a4c-a2ec-767a092ba068"
      },
      "source": [
        "One could also implement the human in the loop by creating a separate Workflow just for gathering human input and making use of nested Workflows. This design could be used in situations where you would want the human input gathering to be a separate service from the rest of the Workflow, which is what would happen if you deployed the nested workflows with llama-deploy."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llama-index-core",
      "language": "python",
      "name": "llama-index-core"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}